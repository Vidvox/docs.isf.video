(window.webpackJsonp=window.webpackJsonp||[]).push([[10],{175:function(e,t,n){"use strict";n.r(t);var r=n(0),a=Object(r.a)({},function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"content"},[e._m(0),e._v(" "),n("p",[e._v("In image filtering one of the commonly used techniques from mathematics is are convolution matrices, also sometimes called a kernel.  Convolution is a process by which an element, in this case a pixel, is adjusted by performing some sort of function along with its neighboring pixels.  This is generally used for blurring, sharpening, embossing, edge detection, and other situations where multiple pixels are being blended together in some fashion.  This lesson on convolution will be relevant whether you are writing shaders to meet the ISF specification and for using GLSL other environments.")]),e._v(" "),n("p",[e._v("For this chapter we'll look at:")]),e._v(" "),e._m(1),e._v(" "),n("p",[e._v("These concepts will also come into play in later chapters as we dive further into advanced topics of GLSL and ISF.")]),e._v(" "),e._m(2),e._v(" "),n("p",[e._v("In mathematics, a convolution matrix, or kernel, is a set of weights that describe how a number of elements are to be added together.  For our purposes each kernel is a 3x3 grid of numbers, but in some cases you may encounter 5x5 or even larger kernels.")]),e._v(" "),n("p",[e._v("An example kernel may look like:")]),e._v(" "),e._m(3),e._m(4),e._v(" "),e._m(5),e._v(" "),e._m(6),e._v(" "),e._m(7),e._v(" "),n("p",[e._v("Many different effects can be created by using different weight values as inputs.  To get a sense for how some of the most important image filters works let's look at some sample kernels.")]),e._v(" "),e._m(8),e._v(" "),e._m(9),e._m(10),e._v(" "),e._m(11),e._v(" "),e._m(12),e._v(" "),e._m(13),n("p",[e._v("Though box blurs and other similar blurs are easily created with convolutions, a single pass of a 3x3 kernel does not produce a particularly deep blurring effect.  In the next chapter covering multi-pass shaders we'll look at how performing these kernels more than once can greatly increase the depth of the blur.")]),e._v(" "),e._m(14),e._v(" "),n("p",[e._v("Often considered the opposite of the blur is the sharpen, which subtracts values on the diagonals instead of averaging.  This can often make edges appear more pronounced.")]),e._v(" "),e._m(15),n("p",[e._v("Like with blurs, increasing the weight of the middle pixel compared to the amount subtracted will result in a stronger sharpening effect.  Similarly the sum of the weights must always be 1.0 for sharpen filters.")]),e._v(" "),e._m(16),e._v(" "),n("p",[e._v("In a similar fashion to sharpen kernels for edge detection the trick is to subtract.  However in this case, instead of the the sum of all of the weights being 1.0, they'll come out to 0.0;  this way it requires that certain pixels be much brighter for others to stay in the image, while most of them are set to black.")]),e._v(" "),e._m(17),e._m(18),n("p",[e._v("Just as there are many ways to write sharpen and blur filters with varying amount of strength based on the relative weights, the same is true of edge detection kernels.")]),e._v(" "),e._m(19),e._v(" "),n("p",[e._v("When creating GLSL shaders that evaluate convolution kernels it can be useful to make use of the vertex shader stage when possible.  Like with rotations this allows us to prepare coordinate values that are used for lookup during the fragment shader stage, saving valuable GPU time during rendering.")]),e._v(" "),e._m(20),e._v(" "),n("p",[e._v("First we'll make the vertex shader for the convolution kernel.  Whether you are creating a generalized use case like in this example, or creating a shader based on a specific kernel, your vertex shader will likely look something like this:")]),e._v(" "),e._m(21),e._m(22),e._v(" "),e._m(23),e._v(" "),n("p",[e._v("Now we can create the fragment shader that performs the actual convolution.  For this generalized shader we declare 9 float values, one for each weight and give each a range of -8.0 to 8.0.  Only the middle pixel is set to 1.0 by default, so when first loaded the filter will function as a pass-thru.")]),e._v(" "),e._m(24),e._m(25),e._v(" "),n("p",[e._v("Within the main() {} function the initial chunk of code gathers each neighboring pixel.  Next the convolution result vec3 RGB color is created by multiplying each pixel by its weight and adding them all together.  One small detail is that we use the alpha channel from the original pixel.")]),e._v(" "),e._m(26),e._v(" "),n("p",[e._v("As you might imagine, getting a specific look by manipulating 9 different sliders can be a bit much.  This is why often it can be useful to pick a particular kernel and create a specific filter that includes a one or two high level parameters which modify the weights to adjust the strength.")]),e._v(" "),e._m(27),e._v(" "),e._m(28),e._v(" "),n("p",[e._v("A very simple blur filter that has a single strength value can be written as such.")]),e._v(" "),e._m(29),e._m(30),e._v(" "),n("p",[e._v("As a challenge, try adapting one of the other kernels we looked at, such as the sharpen kernel, and adapt this code to apply it instead of the box blur.")]),e._v(" "),e._m(31),e._v(" "),n("p",[e._v("In the examples so far we've used a vertex shader to pre-compute our coordinate points used in our fragment shaders.  While this is recommended when possible, there are times when your algorithm may use an indeterminate number of lookup points, or the lookup points may vary depending on other factors within your fragment shader code.")]),e._v(" "),e._m(32),n("p",[e._v("Here instead of using the vertex shader to pre-compute the lookup points and pass them over with varying variables, the lookup points are computed each time inside of the for loop.  With this method it would be easy to adapt this blur to doing a 5x5 neighbor pixel averaging without having to declare any new variables.")]),e._v(" "),n("p",[e._v("As a note, it is recommended to avoid doing too many pixel lookups and comparisons within your code.  While there is no technical limit on this these operations are comparatively costly.  In some cases you can write in optimizations, such as for situations where you know a weight value for a pixel is 0.0, you can skip its corresponding lookup.")]),e._v(" "),e._m(33),e._v(" "),n("p",[e._v("A number of the sample shaders on the "),n("a",{attrs:{href:"http://interactiveshaderformat.com",target:"_blank",rel:"noopener noreferrer"}},[e._v("ISF Sharing Website"),n("OutboundLink")],1),e._v(" are great examples of convolution filters.  As hinted at, many of the convolution shaders that are used in actual practice use another advanced technique that we will soon learn about for creating ISF compositions with multiple render passes.")]),e._v(" "),e._m(34),e._v(" "),n("p",[e._v("Though not technically considered convolution filters because they don't just simply apply a kernel to a set of pixels, there are many visual effects that can be created by comparing a pixel to its neighbors.  Once you have the data in vec4 variables you can apply whatever functions or operations you can think of to combine the values into a single result.")]),e._v(" "),n("p",[e._v("Here are a few filters in particular that use a similar technique:")]),e._v(" "),e._m(35),e._v(" "),n("p",[e._v("Examples of these shaders and other related examples can be found on "),n("a",{attrs:{href:"http://interactiveshaderformat.com",target:"_blank",rel:"noopener noreferrer"}},[e._v("ISF Sharing Website"),n("OutboundLink")],1),e._v(".")])])},[function(){var e=this.$createElement,t=this._self._c||e;return t("h1",{attrs:{id:"convolution-filter"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#convolution-filter","aria-hidden":"true"}},[this._v("#")]),this._v(" Convolution Filter")])},function(){var e=this.$createElement,t=this._self._c||e;return t("ul",[t("li",[this._v("What a convolution matrix is and how they are used to process images.")]),this._v(" "),t("li",[this._v("How to set up a generalized convolution ISF filter.")]),this._v(" "),t("li",[this._v("How to build specific use cases of convolution filters.")]),this._v(" "),t("li",[this._v("Discuss other similar use cases.")])])},function(){var e=this.$createElement,t=this._self._c||e;return t("h2",{attrs:{id:"what-is-a-convolution-matrix"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#what-is-a-convolution-matrix","aria-hidden":"true"}},[this._v("#")]),this._v(" What is a convolution matrix?")])},function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[this._v("//\tA blurring kernel\n[0.0625, 0.125, 0.0625,\n0.125, 0.25, 0.125,\n0.0625, 0.125, 0.0625]\n")])])])},function(){var e=this.$createElement,t=this._self._c||e;return t("p",[this._v("Each of the numbers in this grid is called a "),t("code",[this._v("weight")]),this._v(".")])},function(){var e=this.$createElement,t=this._self._c||e;return t("p",[this._v("Here you can imagine the middle pixel as the one being evaluated.  In our GLSL code we so far have retrieved this by using the "),t("code",[this._v("IMG_THIS_PIXEL()")]),this._v(" function on an image.  To obtain the output result, we obtain the values of all of the neighboring pixels and combine them using the weights as multipliers.  When this operation is performed for every pixel in your image, you can get wildly different results by changing these weight values.")])},function(){var e=this.$createElement,t=this._self._c||e;return t("pre",[t("code",[this._v("Note: For further reading on kernels visit the [Wikipedia page on convolution for image processing](https://en.wikipedia.org/wiki/Kernel_(image_processing)).\n")])])},function(){var e=this.$createElement,t=this._self._c||e;return t("h3",{attrs:{id:"example-kernels"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#example-kernels","aria-hidden":"true"}},[this._v("#")]),this._v(" Example Kernels")])},function(){var e=this.$createElement,t=this._self._c||e;return t("h4",{attrs:{id:"the-identity-kernel"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#the-identity-kernel","aria-hidden":"true"}},[this._v("#")]),this._v(" The Identity Kernel")])},function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[this._v("//\tThe Identity Kernel\n[0.0, 0.0, 0.0,\n0.0, 1.0, 0.0,\n0.0, 0.0, 0.0]\n")])])])},function(){var e=this.$createElement,t=this._self._c||e;return t("p",[this._v("In this case each neighboring pixel is multiplied by 0.0 and the middle pixel is multiplied by 1.0, so the output is simply equal to the middle pixel.  When using this special kernel, known as the "),t("code",[this._v("Identity")]),this._v(", the output will always look the same as the input.")])},function(){var e=this.$createElement,t=this._self._c||e;return t("h4",{attrs:{id:"the-box-blur-kernel"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#the-box-blur-kernel","aria-hidden":"true"}},[this._v("#")]),this._v(" The Box Blur Kernel")])},function(){var e=this.$createElement,t=this._self._c||e;return t("p",[this._v("Another commonly found kernel is the one for a "),t("code",[this._v("Box Blur")]),this._v(".  Many blur filters are based on this idea of averaging the middle pixel with the values around it.  The stronger the weights of the neighboring pixels in comparison to the middle, the stronger the blur.  In such cases it is important for the sum of all of the weights to be 1.0.")])},function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[this._v("//\tThe Box Blur Kernel\n[0.11111, 0.11111, 0.11111,\n0.11111, 0.11111, 0.11111,\n0.11111, 0.11111, 0.11111]\n")])])])},function(){var e=this.$createElement,t=this._self._c||e;return t("h4",{attrs:{id:"sharpen-kernel"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#sharpen-kernel","aria-hidden":"true"}},[this._v("#")]),this._v(" Sharpen Kernel")])},function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[this._v("//\tThe Sharpen Kernel\n[-1.0, 0.0, -1.0,\n0.0, 5.0, 0.0,\n-1.0, 0.0, -1.0]\n")])])])},function(){var e=this.$createElement,t=this._self._c||e;return t("h4",{attrs:{id:"edge-detection-kernels"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#edge-detection-kernels","aria-hidden":"true"}},[this._v("#")]),this._v(" Edge Detection Kernels")])},function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[this._v("//\tEdge Detection 1\n[1.0, 0.0, 1.0,\n0.0, -4.0, 0.0,\n1.0, 0.0, 1.0]\n")])])])},function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[this._v("//\tEdge Detection 2\n[-1.0, -1.0, -1.0,\n-1.0, 8.0, -1.0,\n-1.0, -1.0, 1.0]\n")])])])},function(){var e=this.$createElement,t=this._self._c||e;return t("h2",{attrs:{id:"writing-a-generalized-convolution-filter-in-isf"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#writing-a-generalized-convolution-filter-in-isf","aria-hidden":"true"}},[this._v("#")]),this._v(" Writing a Generalized Convolution Filter in ISF")])},function(){var e=this.$createElement,t=this._self._c||e;return t("h3",{attrs:{id:"create-the-vertex-shader"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#create-the-vertex-shader","aria-hidden":"true"}},[this._v("#")]),this._v(" Create the Vertex Shader")])},function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[this._v("varying vec2 left_coord;\nvarying vec2 right_coord;\nvarying vec2 above_coord;\nvarying vec2 below_coord;\n\nvarying vec2 lefta_coord;\nvarying vec2 righta_coord;\nvarying vec2 leftb_coord;\nvarying vec2 rightb_coord;\n\nvoid main()\n{\n\tisf_vertShaderInit();\n\tvec2 texc = vec2(isf_FragNormCoord[0],isf_FragNormCoord[1]);\n\tvec2 d = 1.0/RENDERSIZE;\n\n\tleft_coord = clamp(vec2(texc.xy + vec2(-d.x , 0)),0.0,1.0);\n\tright_coord = clamp(vec2(texc.xy + vec2(d.x , 0)),0.0,1.0);\n\tabove_coord = clamp(vec2(texc.xy + vec2(0,d.y)),0.0,1.0);\n\tbelow_coord = clamp(vec2(texc.xy + vec2(0,-d.y)),0.0,1.0);\n\n\tlefta_coord = clamp(vec2(texc.xy + vec2(-d.x , d.x)),0.0,1.0);\n\trighta_coord = clamp(vec2(texc.xy + vec2(d.x , d.x)),0.0,1.0);\n\tleftb_coord = clamp(vec2(texc.xy + vec2(-d.x , -d.x)),0.0,1.0);\n\trightb_coord = clamp(vec2(texc.xy + vec2(d.x , -d.x)),0.0,1.0);\n}\n")])])])},function(){var e=this.$createElement,t=this._self._c||e;return t("p",[this._v("Here we've declared eight different varying vec2 variables, one for each of the neighboring pixels.  Combined with the middle coordinate itself located at "),t("code",[this._v("isf_FragNormCoord")]),this._v(" this completes the calculations needed to do the image pixel lookups in the fragment shader stage.")])},function(){var e=this.$createElement,t=this._self._c||e;return t("h3",{attrs:{id:"create-the-fragment-shader"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#create-the-fragment-shader","aria-hidden":"true"}},[this._v("#")]),this._v(" Create the Fragment Shader")])},function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[this._v('/*{\n\t"CREDIT": "by VIDVOX",\n\t"ISFVSN": "2",\n\t"CATEGORIES": [\n\t\t"Blur"\n\t],\n\t"INPUTS": [\n\t\t{\n\t\t\t"NAME": "inputImage",\n\t\t\t"TYPE": "image"\n\t\t},\n\t\t{\n\t\t\t"NAME": "w00",\n\t\t\t"TYPE": "float",\n\t\t\t"MIN": -8.0,\n\t\t\t"MAX": 8.0,\n\t\t\t"DEFAULT": 0.0\n\t\t},\n\t\t{\n\t\t\t"NAME": "w10",\n\t\t\t"TYPE": "float",\n\t\t\t"MIN": -8.0,\n\t\t\t"MAX": 8.0,\n\t\t\t"DEFAULT": 0.0\n\t\t},\n\t\t{\n\t\t\t"NAME": "w20",\n\t\t\t"TYPE": "float",\n\t\t\t"MIN": -8.0,\n\t\t\t"MAX": 8.0,\n\t\t\t"DEFAULT": 0.0\n\t\t},\n\t\t{\n\t\t\t"NAME": "w01",\n\t\t\t"TYPE": "float",\n\t\t\t"MIN": -8.0,\n\t\t\t"MAX": 8.0,\n\t\t\t"DEFAULT": 0.0\n\t\t},\n\t\t{\n\t\t\t"NAME": "w11",\n\t\t\t"TYPE": "float",\n\t\t\t"MIN": -8.0,\n\t\t\t"MAX": 8.0,\n\t\t\t"DEFAULT": 1.0\n\t\t},\n\t\t{\n\t\t\t"NAME": "w21",\n\t\t\t"TYPE": "float",\n\t\t\t"MIN": -8.0,\n\t\t\t"MAX": 8.0,\n\t\t\t"DEFAULT": 0.0\n\t\t},\n\t\t{\n\t\t\t"NAME": "w02",\n\t\t\t"TYPE": "float",\n\t\t\t"MIN": -8.0,\n\t\t\t"MAX": 8.0,\n\t\t\t"DEFAULT": 0.0\n\t\t},\n\t\t{\n\t\t\t"NAME": "w12",\n\t\t\t"TYPE": "float",\n\t\t\t"MIN": -8.0,\n\t\t\t"MAX": 8.0,\n\t\t\t"DEFAULT": 0.0\n\t\t},\n\t\t{\n\t\t\t"NAME": "w22",\n\t\t\t"TYPE": "float",\n\t\t\t"MIN": -8.0,\n\t\t\t"MAX": 8.0,\n\t\t\t"DEFAULT": 0.0\n\t\t}\n\t]\n}*/\n\n\nvarying vec2 left_coord;\nvarying vec2 right_coord;\nvarying vec2 above_coord;\nvarying vec2 below_coord;\n\nvarying vec2 lefta_coord;\nvarying vec2 righta_coord;\nvarying vec2 leftb_coord;\nvarying vec2 rightb_coord;\n\n\nvoid main()\n{\n\tvec4 colorLA = IMG_NORM_PIXEL(inputImage, lefta_coord);\n\tvec4 colorA = IMG_NORM_PIXEL(inputImage, above_coord);\n\tvec4 colorRA = IMG_NORM_PIXEL(inputImage, righta_coord);\n\n\tvec4 colorL = IMG_NORM_PIXEL(inputImage, left_coord);\n\tvec4 color = IMG_THIS_NORM_PIXEL(inputImage);\n\tvec4 colorR = IMG_NORM_PIXEL(inputImage, right_coord);\n\t\n\tvec4 colorLB = IMG_NORM_PIXEL(inputImage, leftb_coord);\n\tvec4 colorB = IMG_NORM_PIXEL(inputImage, below_coord);\n\tvec4 colorRB = IMG_NORM_PIXEL(inputImage, rightb_coord);\n\n\t//\tmake the average for the RGB values\n\tvec3 convolution = (w11 * color + w01 * colorL + w21 * colorR + w10 * colorA + w12 * colorB + w00 * colorLA + w20 * colorRA + w02 * colorLB + w22 * colorRB).rgb;\n\t\n\t//\tkeep the alpha the same as the original pixel\n\tgl_FragColor = vec4(convolution,color.a);\n}\n')])])])},function(){var e=this.$createElement,t=this._self._c||e;return t("p",[this._v("In the code section below the JSON, we can see that the "),t("code",[this._v("varying")]),this._v(" vec2 variables from the vertex shader have also been declared here.  This makes it possible for those values to be read here in the fragment shader.")])},function(){var e=this.$createElement,t=this._self._c||e;return t("h2",{attrs:{id:"creating-specific-kernels-as-filters"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#creating-specific-kernels-as-filters","aria-hidden":"true"}},[this._v("#")]),this._v(" Creating Specific Kernels As Filters")])},function(){var e=this.$createElement,t=this._self._c||e;return t("p",[this._v("Note that for these examples we will use a vertex shader identical to the one used above for the general convolution case.  Be sure to duplicate that code in its own file with a name that matches your fragment shaders below and uses the "),t("code",[this._v(".vs")]),this._v(" extension.")])},function(){var e=this.$createElement,t=this._self._c||e;return t("h3",{attrs:{id:"creating-a-varying-box-blur-filter-in-isf"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#creating-a-varying-box-blur-filter-in-isf","aria-hidden":"true"}},[this._v("#")]),this._v(" Creating a Varying Box Blur Filter in ISF")])},function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[this._v('/*{\n\t"CREDIT": "by VIDVOX",\n\t"ISFVSN": "2",\n\t"CATEGORIES": [\n\t\t"Blur"\n\t],\n\t"INPUTS": [\n\t\t{\n\t\t\t"NAME": "inputImage",\n\t\t\t"TYPE": "image"\n\t\t},\n\t\t{\n\t\t\t"NAME": "blurLevel",\n\t\t\t"TYPE": "float",\n\t\t\t"MIN": 0.0,\n\t\t\t"MAX": 1.0,\n\t\t\t"DEFAULT": 0.0\n\t\t}\n\t]\n}*/\n\nvarying vec2 left_coord;\nvarying vec2 right_coord;\nvarying vec2 above_coord;\nvarying vec2 below_coord;\n\nvarying vec2 lefta_coord;\nvarying vec2 righta_coord;\nvarying vec2 leftb_coord;\nvarying vec2 rightb_coord;\n\nvoid main()\n{\n\n\tfloat mWeight = 1.0 - blurLevel;\n\tfloat nWeight = blurLevel / 8.0;\n\t\n\tvec4 color = IMG_THIS_NORM_PIXEL(inputImage);\n\t\n\t//\tnote that we can skip the pixel lookups here if nWeight is 0.0\n\tvec4 colorL = (nWeight > 0.0) ? IMG_NORM_PIXEL(inputImage, left_coord) : vec4(0.0);\n\tvec4 colorR = (nWeight > 0.0) ? IMG_NORM_PIXEL(inputImage, right_coord) : vec4(0.0);\n\tvec4 colorA = (nWeight > 0.0) ? IMG_NORM_PIXEL(inputImage, above_coord) : vec4(0.0);\n\tvec4 colorB = (nWeight > 0.0) ? IMG_NORM_PIXEL(inputImage, below_coord) : vec4(0.0);\n\n\tvec4 colorLA = (nWeight > 0.0) ? IMG_NORM_PIXEL(inputImage, lefta_coord) : vec4(0.0);\n\tvec4 colorRA = (nWeight > 0.0) ? IMG_NORM_PIXEL(inputImage, righta_coord) : vec4(0.0);\n\tvec4 colorLB = (nWeight > 0.0) ? IMG_NORM_PIXEL(inputImage, leftb_coord) : vec4(0.0);\n\tvec4 colorRB = (nWeight > 0.0) ? IMG_NORM_PIXEL(inputImage, rightb_coord) : vec4(0.0);\n\n\tvec3 blur = ((mWeight * color) + nWeight * (colorL + colorR + colorA + colorB + colorLA + colorRA + colorLB + colorRB)).rgb;\n\t\n\tgl_FragColor = vec4(blur,color.a);\n}\n')])])])},function(){var e=this.$createElement,t=this._self._c||e;return t("p",[this._v("Unlike our previous box blur kernel which had fixed values, this determines its weights based on the "),t("code",[this._v("blurLevel")]),this._v(" uniform variable declared in the JSON section.  As that value moves from 0.0 to 1.0, the weight of the neighbors increases while the weight of the middle pixel decreases.  Recall that for a blur or sharpen we need the sum of our weights to be 1.0.")])},function(){var e=this.$createElement,t=this._self._c||e;return t("h3",{attrs:{id:"creating-convolution-filters-with-for-loops"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#creating-convolution-filters-with-for-loops","aria-hidden":"true"}},[this._v("#")]),this._v(" Creating Convolution Filters With For Loops")])},function(){var e=this.$createElement,t=this._self._c||e;return t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[this._v('/*\n{\n  "CATEGORIES" : [\n    "Blur"\n  ],\n  "DESCRIPTION" : "",\n  "ISFVSN" : "2",\n  "INPUTS" : [\n    {\n      "NAME" : "inputImage",\n      "TYPE" : "image"\n    },\n    {\n      "NAME" : "blurLevel",\n      "TYPE" : "float",\n      "MAX" : 1,\n      "DEFAULT" : 0.5,\n      "MIN" : 0\n    }\n  ],\n  "CREDIT" : "by VIDVOX"\n}\n*/\n\nvoid main()\t{\n\tfloat mWeight = 1.0 - blurLevel;\n\tfloat nWeight = blurLevel / 8.0;\n\tvec4 result = vec4(0.0);\n\tif (blurLevel > 0.0)\t{\n\t\tfor (int i = -1;i <= 1;++i)\t{\n\t\t\tfor (int j = -1;j <= 1;++j)\t{\n\t\t\t\tvec2 loc = gl_FragCoord.xy + vec2(i,j);\n\t\t\t\tvec4 color = IMG_PIXEL(inputImage,loc);\n\t\t\t\tif ((i == 0)&&(j == 0))\t{\n\t\t\t\t\tresult.rgb += mWeight * color.rgb;\n\t\t\t\t\tresult.a = color.a;\n\t\t\t\t}\n\t\t\t\telse\t{\n\t\t\t\t\tresult.rgb += nWeight * color.rgb;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\telse\t{\n\t\tresult = IMG_THIS_NORM_PIXEL(inputImage);\t\n\t}\n\t\n\tgl_FragColor = result;\n}\n')])])])},function(){var e=this.$createElement,t=this._self._c||e;return t("h3",{attrs:{id:"more-examples-of-convolution-shaders"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#more-examples-of-convolution-shaders","aria-hidden":"true"}},[this._v("#")]),this._v(" More Examples Of Convolution Shaders")])},function(){var e=this.$createElement,t=this._self._c||e;return t("h2",{attrs:{id:"other-similar-use-cases"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#other-similar-use-cases","aria-hidden":"true"}},[this._v("#")]),this._v(" Other Similar Use Cases")])},function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ul",[n("li",[e._v("An "),n("code",[e._v("Erode")]),e._v(" effect searches surrounding pixels looking for minimum values.")]),e._v(" "),n("li",[e._v("A "),n("code",[e._v("Dilate")]),e._v(" effect searches surrounding pixels looking for maximum values.")]),e._v(" "),n("li",[e._v("A "),n("code",[e._v("Frosted Glass")]),e._v(" style effect can blend together several local pixels in a non-standard pattern.")]),e._v(" "),n("li",[e._v("An "),n("code",[e._v("Emboss")]),e._v(" effect uses a combination of a convolution kernel and other post processing to create its output.")])])}],!1,null,null,null);a.options.__file="primer_chapter_6.md";t.default=a.exports}}]);